---
title: "hierarchical models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Bayesian inference

We decided to switch to Bayesian inference to be able to better take into account uncertainty in model fitting. 

## Hieararchical models

We want to acknowledge for features shared by subpopulations. We want somehow to take into account distinct grpups' membership. 

**Hierarchical models** are extensions of regression models in which data are structured in groups and coefficients can vary by group. 

In our case we want to model the fact that different people live in different departments, we have some information at individual level, and some are available (and constant) at department level. 

So our data are structured in a hierarchical way: people within departments. 

We extend the Negative Binomial model encoding hierarchical structure by allowing the intercept to vary from department to department. 

In bayesian context, we can check a model by **posterior predictive checking** (asses the fit of a model to data) to check the joint posterior predictive distribution of future data given the data at hand.  The idea is that replicated data under the model should look similar to the observed data (plausible under the posterior predictive distribution).

The basic technique for checking the fit of a model is to draw simulated values from the joint posterior predictive distribution of replicated data and compare these samples to the observed data. Any sistematic difference between the simulation and the data indicate potential failing of the model. 

$y_{rep}$ are replicated data that could have been observed, **in-sample** replication. The posterior predictive distribution of $y_{rep}$ given our data is $p(y_{rep}|y) = \int p(y_{rep}|\theta)\pi (\theta|y) d\theta$.
 
## looic

The goal is to asses model comparison, the idea is that **distinct models represent distinct hypothesis**.

We want to evaluate the prediction accuracy both to measure the performance of a model that we are using and to compare models. 

It is an extention of AIC based on cross validation.

The general form of an information criteria is $-2lpd + penalty$, where `lpd` is a measure of the log predictive density of the fitted model and `penalty` is a penalization accounting for the effective number of parameters of the fitted model. 

The general idea is: the lower is a particular value for an information criteria, the better is the model fit. 

The purpose of LOO is to estimate the accuracy of the predictive distribution.

We computed and stored the pointwise log-likelihood inside the Stan model.

LOO is a method for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. 

The Bayesian LOO estimate of the out-of-sample predictive fit is:

(`elpd` stands for expected log pointwise predictive density for a new dataset, while p(y_i|y_{-i}) is the leave-one-out predictive density given the data withiut the i-th data point)

$$
elpd_{loo} = \sum_{i=1}^{n}log (p(y_i|y_{-i})) \\
p(y_i|y_{-i}) = \int p(y_i|\theta)p(\theta|y_{-i})d\theta
$$

[source for the formula](http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf)

## Estimated MSE with stan 

## Choice of the priors
