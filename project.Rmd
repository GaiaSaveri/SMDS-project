---
title: "Colombia COVID-19 - Central region"
author: "Angela Carraro, Giullia Monteiro Milano Oliveira, Gaia Saveri"
date: "3/07/2020"
output:
  rmdformats::readthedown:
  html_document:
    highlight: kate
    lightbox: true
    gallery: true
    toc: yes
    toc_depth: 3
  ioslides_presentation:
    highlight: kate
  include: null
  beamer_presentation:
    highlight: kate
  pdf_document:
    highlight: kate
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: kate
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```

```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
```

```{r message=FALSE}
library(MASS)
library(readr)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(ggrepel)
library(bayesplot)
library(RColorBrewer)
library(leaflet)
library(geojsonio)
library(htmltools)
library(htmlwidgets)
library(rstan)
```

# Colombia COVID-19

**LINK:** https://www.kaggle.com/camesruiz/colombia-covid19-complete-dataset

**DESCRIPTION:** Coronavirus (COVID-19) made its outbreak in Colombia with the first confirmed in the country on march 6th, since then, number of confirmed cases has been increasing and deaths related to the virus are starting to have the first confirmed cases. This data set contains complete information about confirmed cases, deaths and number of recovered patients according to the daily reports by the colombian health department (Ministerio de Salud)

**GOAL:** Build a model for the number of `confirmed` cases in the different Colombia regions. You have the access to some covariates, such as: `Edad` (age), `Sexo` (Sex), `Pais de procedencia` (origin country) of the individual cases. Try to test the predictive accuracy of your selected model.

**ATTENTION:** Three countries are here considered: Colombia, Mexico and India. Each different group of students should focus on a geographical sub-area of one of the three countries, say the northern, the central or the southern part of the countries, by pooling all the regions/states/departments belonging to the considered area. Say: group A focuses on Northern Mexico, group B on Central Mexico, and so on. The distinction in northern, central and southern is not strict, you have some flexibility.

----

# Our Project

We decided to do central Colombia because it is where the capital is.
 
The largest cities in the country are Bogotá (in the center), Medellín (in the north, close to central), Cali (in the center) and Barranquilla (extreme north).

## Dataset - First Exploration

```{r load_data, echo=TRUE}
colombia_covid <- read_csv("data/colombia_covid.csv")
unique(colombia_covid$`Departamento o Distrito`)
#options(tibble.print_max = Inf) # to show all the elements of the list, but set it also for other chunks
#options(tibble.width = Inf)
colombia_covid %>%
  group_by(`Departamento o Distrito`) %>%
  count()
```

Colombia is divided into 32 departments. According to [Wikipedia](https://en.wikipedia.org/wiki/Departments_of_Colombia) we miss the Departments of Amazonas, Arauca, Caquetá, Chocó, Guainía, Guaviare, Magdalena, Putumayo, Vaupés, Vichada.

Bogotá, Distrito Capital in in the Cundinamarca Department.
Barranquilla D.E. is a "Distrito Especial" but should be in the Atlántico Department. 

The Districts (Spanish: Distrito) in Colombia are cities that have a feature that highlights them, such as its location and trade, history or tourism. Arguably, the districts are special municipalities. The districts are Bogotá, Barranquilla, Cartagena, Santa Marta, Cúcuta, Popayán, Tunja, Buenaventura, Turbo and Tumaco.

We miss Cúcuta, Popayán, Tunjaa, Buenaventura, Turbo and Tumaco.

```{r create_gis_df, echo=TRUE}
#lat-long
#bogota<c(4.592164298, -74.072166378, 542)
valle_de_cauca<-c("Valle del Cauca", 3.4372200, -76.5225000, 150)
cauca<-c("Cauca", 2.43823, -76.6131592, 12) 
antioquia<-c("Antioquia",6.2518400, -75.5635900, 127)
cartagena<-c("Cartagena D.T. y C", 10.39972, -75.51444, 39)
huila<-c("Huila", 2.9273, -75.2818909, 30)
meta<-c("Meta", 4.1420000, -73.6266400, 12)
risaralda<-c("Risaralda", 4.8133302, -75.6961136, 35)
norte_santander<-c("Norte de Santander", 7.8939100, -72.5078200, 21)
caldas<-c("Caldas", 5.0688900, -75.5173800, 16)
cudinamarca<-c("Cundinamarca", 4.862437, -74.058655, 42)
barraquilla<-c("Barranquilla D.E.", 10.9685400, -74.7813200, 35) #atlantico
santader<-c("Santander", 7.1253900, -73.1198000, 12)
quindio<-c("Quindío", 4.535000, -75.675690, 23)
tolima<-c("Tolima", 4.43889, -75.2322235, 14)
santa_marta<-c("Santa Marta D.T. y C.", 11.24079, -74.19904, 12)
cesar<-c("Cesar", 10.4631400, -73.2532200, 16)
san_andres<-c("San Andrés", 12.5847197, -81.7005615, 2)
casanare<-c("Casanare", 5.3377500, -72.3958600, 2)
narino<-c("Nariño", 1.2136100, -77.2811100, 6)
boyaca<-c("Boyacá", 5.767222, -72.940651, 6)
cordoba<-c("Córdoba", 8.7479800, -75.8814300, 2)
bolivar<-c("Bolívar", 10.3997200, -75.5144400, 3)
sucre<-c("Sucre", 9.3047199, -75.3977814, 1)
guajira<-c("La Guajira", 11.5444400, -72.9072200, 1)

gis_data<-data.frame(name="Bogotá D.C.", latitude=4.624335, longitude=-74.063644, cases=542) #bogotà
gis_data$name<-as.character(gis_data$name)
gis_data<-rbind(gis_data, cauca)
gis_data<-rbind(gis_data, valle_de_cauca)
gis_data<-rbind(gis_data, antioquia)
gis_data<-rbind(gis_data, cartagena)
gis_data<-rbind(gis_data, huila)
gis_data<-rbind(gis_data, meta)
gis_data<-rbind(gis_data, risaralda)
gis_data<-rbind(gis_data, norte_santander)
gis_data<-rbind(gis_data, caldas)
gis_data<-rbind(gis_data, cudinamarca)
gis_data<-rbind(gis_data, barraquilla)
gis_data<-rbind(gis_data, santader)
gis_data<-rbind(gis_data, quindio)
gis_data<-rbind(gis_data, tolima)
gis_data<-rbind(gis_data, santa_marta)
gis_data<-rbind(gis_data, cesar)
gis_data<-rbind(gis_data, san_andres)
gis_data<-rbind(gis_data, casanare)
gis_data<-rbind(gis_data, narino)
gis_data<-rbind(gis_data, boyaca)
gis_data<-rbind(gis_data, cordoba)
gis_data<-rbind(gis_data, bolivar)
gis_data<-rbind(gis_data, sucre)
gis_data<-rbind(gis_data, guajira)

gis_data$latitude<-as.numeric(gis_data$latitude)
gis_data$longitude<-as.numeric(gis_data$longitude)
gis_data$cases<-as.numeric(gis_data$cases)

```

```{r create_maps,echo=TRUE}
getColor <- function(gis_data) {
  sapply(gis_data$cases, function(cases) {
  if(cases <= 10) {
    "green"
  } else if(cases <= 100) {
    "orange"
  } else {
    "red"
  } })
}

icons <- awesomeIcons(
  icon = 'ios-close',
  iconColor = 'black',
  library = 'ion',
  markerColor = getColor(gis_data)
)

dept<-geojsonio::geojson_read("data/Colombia.geo.json", what="sp")

labels <- sprintf(
  "<strong>%s</strong><br/>",
  dept$NOMBRE_DPT
) %>% lapply(htmltools::HTML)

m <- leaflet(data=gis_data) %>% addTiles() %>%
  addAwesomeMarkers(~longitude, ~latitude, icon=icons, label = ~as.character(cases)) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(data=dept,     
              opacity = 0.2,
              color = "yellow",
              dashArray = "3",
              fillOpacity = 0.1,
              highlight = highlightOptions(
                weight = 5,
                color = "#666",
                dashArray = "",
                fillOpacity = 0.7,
                bringToFront = TRUE),
                label = labels)
m
#saveWidget(m, file="figs/colombia.html")
```

The color of the pins is related with the number of cases: if they are less than 10 the color is "green", if they are less than 100 the color is "orange", otherwise it is "red".\
On the map there are all the cities/departments for which we have data. We can notice that we don't have any data in the south of the country.

Reading here and there I found that Colombia in divided in 5 regions, the central one comprises: Boyacá, Tolima, Cundinamarca, Meta, Bogotà DC.

**ANGELA:** Seeing [Wikipedia](https://en.wikipedia.org/wiki/Natural_regions_of_Colombia) I think that the Orinoquía Region (Meta, Arauca, Casanare and Vichada Departments) is in the center, so I would add also **Arauca**, **Casanare** and **Vichada**. I noticed that we only have Casanare, the other two doesn't have data.

**GAIA**: added Casanare, for the others we have no data!

However, since in our assignment Colombia is divided in 3 parts, I think that we should add some more regions (e.g. Quindío, Valle del Cauca, Risaralda, Celdas, Boyacá and possibly Antioquia and Santander)

```{r central_colombia, echo=TRUE, warning=FALSE}
#slice the main dataset
central.colombia.dep<-c("Bogotá D.C.", "Tolima", "Cundinamarca", "Meta", "Boyacá", "Quindío", "Cauca", "Valle del Cauca", "Risaralda", "Caldas", "Boyacá", "Antioquia", "Santander", "Casanare")
central.colombia.rows<-which(colombia_covid$`Departamento o Distrito` %in% central.colombia.dep)
colombia_covid<-colombia_covid[central.colombia.rows,]

#slice the gis_data dataset
central_gis_data<-gis_data[which(gis_data$name %in% central.colombia.dep),]
#slice the geojson dataset 
central_dept<-c("SANTAFE DE BOGOTA D.C", "TOLIMA", "CUNDINAMARCA", "META", "BOYACA", "QUINDIO", "CAUCA", "VALLE DEL CAUCA", "RISARALDA", "CALDAS", "BOYACA", "ANTIOQUIA", "SANTANDER", "CASANARE")
central.dept<-dept[which(dept@data$NOMBRE_DPT %in% central_dept),]
```

```{r our_map, echo=TRUE, warning=FALSE}
getColor <- function(central_gis_data) {
  sapply(central_gis_data$cases, function(cases) {
  if(cases <= 10) {
    "green"
  } else if(cases <= 100) {
    "orange"
  } else {
    "red"
  } })
}

icons <- awesomeIcons(
  icon = 'ios-close',
  iconColor = 'black',
  library = 'ion',
  markerColor = getColor(central_gis_data)
)
#now colombia is yellow and our departments are red
central_map <- leaflet(data=central_gis_data) %>% addTiles() %>%
  addAwesomeMarkers(~longitude, ~latitude, icon=icons, label = ~htmlEscape(paste(name,cases,sep=" : "))) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(data=dept,
              opacity=0.1,
              color="yellow",
              fillOpacity = 0.1) %>%
  addPolygons(data=central.dept,     
              opacity = 0.2,
              color = "red",
              dashArray = "3",
              fillOpacity = 0.1)
central_map
#saveWidget(central_map, file="figs/central_colombia.html")
```

## Some very basics plots

Let's check the situation (and also the power of ggplot)! 

Scattered infos about pandemic in Colombia (https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Colombia): 

  * the quarantine started on the 20th of March, since our data are from 6th of March to 2nd of April, it is very likeliy that quarantine effects are not witnessed in our data.
  
  * on March the 26th there was a damage in the machine that prepared the samples for processing and subsequent diagnosis of COVID-19, which affected the speed at which results were being produced. This could explain the very low number of confirmed cases.
  
```{r, echo=TRUE, warning=FALSE}
theme_set(theme_classic())
region<-colombia_covid$`Departamento o Distrito`
nrows<-10
df <- expand.grid(y = 1:nrows, x = 1:nrows)
categ_table <- round(table(region) * ((nrows*nrows+1)/(length(region))))
df$category<-factor(rep(names(categ_table), categ_table))
waffle_chart <- ggplot(df, aes(x = x, y = y, fill = df$category)) + 
        geom_tile(color = "black", size = 0.5) +
        scale_x_continuous(expand = c(0, 0)) +
        scale_y_continuous(expand = c(0, 0), trans = 'reverse') +
        scale_fill_brewer(palette = "Set3") +
        labs(title="Frequency of cases across Departments", subtitle="Waffle Chart") + 
        theme(#panel.border = element_rect(size = 2),
        plot.title = element_text(size = rel(1.2)),
              axis.text = element_blank(),
              axis.title = element_blank(),
              axis.ticks = element_blank(),
              legend.title = element_blank(),
              legend.position = "right")
waffle_chart
```

The major number of cases are in the capital Bogotà. 

```{r first_plots, echo=TRUE, warning=FALSE}
theme_set(theme_classic())
#number of cases confirmed day by day

#fix day column in "international" format so that R can fix the sorting properly
colombia_covid$`Fecha de diagnóstico`<-as.Date(colombia_covid$`Fecha de diagnóstico`, format="%d/%m/%Y")

colorCount <- length(unique(colombia_covid$`Departamento o Distrito`)) 
getPalette <- colorRampPalette(brewer.pal(9, "Spectral"))(colorCount)

ggplot(colombia_covid, aes(x = `Fecha de diagnóstico`)) +
  scale_fill_manual(values = getPalette) +
  geom_histogram(aes(fill=`Departamento o Distrito`), width = 0.8, stat="count") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6),
        legend.position = "right") +
  labs(title = "Daily number of confirmed cases", 
       subtitle = "subdivided across departments",
       x = "Date of confirmation",
       fill = "Department")

```

The previous plot represents the daily incidence of the desease across all the departments we are taking into account.

Let's check the general trend by looking at the cumulative number of confirmed cases (again, all "our" departments are taken into account): 

```{r cumulative_plot, waring=FALSE}
colombia_covid$`ID de caso` <- 1:dim(colombia_covid)[1]
#the max of the ID de caso for each date is the cumulative number of cases confirmed up to that date
cumulative <- as.data.frame(colombia_covid %>%
  group_by(`Fecha de diagnóstico`) %>%
  summarise(max(`ID de caso`)))
  
names(cumulative)<-c("Fecha de diagnóstico", "Cumulative confirmed")
cumulative

ggplot(cumulative, aes(x=`Fecha de diagnóstico`, y=`Cumulative confirmed`)) +
  geom_point(size=3) +
  geom_segment(aes(x=`Fecha de diagnóstico`,
                   xend=`Fecha de diagnóstico`,
                   y=0,
                   yend=`Cumulative confirmed`)) +
  labs(title = "Cumulative number of confirmed cases",
       x = "Date of confirmation") +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

Here the growth seems exponential (and this is consistent with the fact that we are studying the early stages of the outbreak).

In order to confirm it we should fit a log-linear model, and check that it produces a constant growth rate (straight line). 

Now let's explore the distribution of cases across genders and age:

```{r gender_plot, echo=TRUE, warning=FALSE}
brks <- seq(-250, 250, 50)
lbls <- as.character(c(seq(-250, 0, 50), seq(50, 250, 50)))

ggplot(data=colombia_covid, aes(x=`Departamento o Distrito`, fill = Sexo)) +  
                              geom_bar(data = subset(colombia_covid, Sexo == "F")) +
                              geom_bar(data = subset(colombia_covid, Sexo == "M"), aes(y=..count..*(-1))) +
                              scale_y_continuous(breaks = brks, labels = lbls) + 
                              coord_flip() +  
                              labs(title="Spread of the desease across genders",
                                   y = "Number of cases",
                                   x = "Department",
                                   fill = "Gender") +
                              theme_tufte() +  
                              theme(plot.title = element_text(hjust = .5), 
                                    axis.ticks = element_blank()) +   
                              scale_fill_brewer(palette = "Dark3")  
```

Maybe in order to study the distribution of ages we should divide the ages in groups, for example 0-18, 18-30, 30-45, 45-60, 60-75, 75+.

```{r age_grps_plot, echo=TRUE, warning=FALSE}
#create column age_group (didn't modify the original dataset though)
age_groups<-colombia_covid %>% mutate(age_group = case_when(Edad <= 18 ~ '0-18',
                                             Edad >= 19  & Edad <= 30 ~ '19-30',
                                             Edad >=  31 & Edad <= 45 ~ '31-45',
                                             Edad >= 46 & Edad <= 60 ~ '46-60',
                                             Edad >=61 & Edad <= 75 ~ '60-75',
                                             Edad >=76 ~ '76+'))

#compute percentage so that we can label more precisely the pie chart
age_groups_pie <- age_groups %>% 
  group_by(age_group) %>%
  count() %>%
  ungroup() %>%
  mutate(per=`n`/sum(`n`)) %>% 
  arrange(desc(age_group))
age_groups_pie$label <- scales::percent(age_groups_pie$per)

age_pie <- ggplot(age_groups_pie, aes(x = "", y = per, fill = factor(age_group))) + 
  geom_bar(stat="identity", width = 1) +
  theme(axis.line = element_blank(), 
        plot.title = element_text(hjust=0.5)) + 
  labs(fill="Age groups", 
       x=NULL, 
       y=NULL, 
       title="Distribution of the desease across ages") +
  coord_polar(theta = "y") +
  #geom_text(aes(x=1, y = cumsum(per) - per/2, label=label))
  geom_label_repel(aes(x=1, y=cumsum(per) - per/2, label=label), size=3, show.legend = F, nudge_x = 0) +
  guides(fill = guide_legend(title = "Group"))
  
age_pie 
```

This is quite surprising.. I expected elder people to be more affected by Covid-19!

The overall life expectancy in Colombia at birth is 74.8 years (71.2 years for males and 78.4 years for females). [Wikipedia](https://en.wikipedia.org/wiki/Colombia#Health)

Instead, the median age of the population in 2015 was 29.5 years (30.4 in 2018, 31.3 in 2020), so it is a Country full of young people! [link](https://www.indexmundi.com/colombia/demographics_profile.html) or [link](https://www.statista.com/statistics/368964/average-age-of-the-population-in-colombia/#:~:text=Median%20age%20of%20the%20population%20in%20Colombia%202015&text=It%20is%20a%20single%20index,Colombian%20population%20was%2029.5%20years.) or [link](https://www.google.com/search?sxsrf=ALeKk01gNyeWpDhExBTxkXs2qF621qi2kA%3A1592863896924&ei=mCzxXtPzN4OzmwXqvbboBA&q=average+age+in+colombia&oq=average+age+in+colombia&gs_lcp=CgZwc3ktYWIQAzIGCAAQBxAeMgUIABDLATIGCAAQCBAeMgYIABAIEB4yBggAEAgQHjIGCAAQCBAeOgQIABBHOggIABAIEAcQHjoICAAQCBANEB5QqqgRWJGwEWC8uBFoAHACeACAAX6IAc8GkgEDMC43mAEAoAEBqgEHZ3dzLXdpeg&sclient=psy-ab&ved=0ahUKEwjTn4G2uJbqAhWD2aYKHeqeDU0Q4dUDCAw&uact=5)

Now we can analyze jointly the distribution of age and sex (sex distribution across group of age):

```{r age-sex_plot, echo=TRUE, warning=FALSE}
keep <- c("Sexo", "age_group")
age_groups<-age_groups[names(age_groups)%in%keep]
age_groups_count<-aggregate(cbind(pop=Sexo)~age_group+Sexo,
                      data=age_groups,
                      FUN = function(x){NROW(x)})
age_groups_count$count <- ifelse(age_groups_count$Sexo == "F", age_groups_count$pop * -1, age_groups_count$pop)
age_groups_count<-as.data.frame(age_groups_count[names(age_groups_count)!="pop"])

ggplot(age_groups_count, aes(x=age_group, y=count, fill=Sexo)) +
  geom_col() + 
  #facet_share(~Sexo, dir = "h") +
  coord_flip(clip="off") +
  theme_minimal() +
  labs(title = "Distribution of sex by age",
       y = "",
       x = "Age group")
```

There isn't much difference between the sexes among the different group of ages, I have the impression that the covariates present in the dataset won't help us!! :(

We are now left to explore the `Tipo` variable:

```{r tipo_plot, echo=TRUE, warning=FALSE}
theme_set(theme_classic())
#renamed the attribute since I had sime problem due to the presence of *
colnames(colombia_covid)[8]<-"tipo"

ggplot(colombia_covid, aes(x = `Fecha de diagnóstico`)) +
  scale_fill_brewer(palette = "Set3") +
  geom_histogram(aes(fill=tipo), width = 0.8, stat="count") +
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
  labs(title = "Daily number of confirmed cases", 
       subtitle = "subdivided across type",
       x = "Date of confirmation",
       fill = "Type")
```

I think that `en estudio` means that it is not clear while the case is imported or not, however it seems like there are more imported cases, we can count them:

```{r tipo2, echo=TRUE, warning=FALSE}
type_pie<- colombia_covid %>% 
  group_by(tipo) %>%
  count() %>%
  ungroup() %>%
  mutate(per=`n`/sum(`n`)) %>% 
  arrange(desc(tipo))
type_pie$label <- scales::percent(type_pie$per)
type_pie<-type_pie[names(type_pie)!="per"]
colnames(type_pie)<-c("tipo", "total_number", "percentage")
type_pie
```

Almost half of the total confirmed cases in our region of interest are imported, and a significant percentage is anknown wheter it is imported or not. Again this is in some sense interesting, but I don't see clearly why this should be helpful in our model! 

```{r tipo3, echo=TRUE, warning=FALSE}
imported<-subset(colombia_covid, tipo=="Importado", select = c("País de procedencia"))
imported %>%
  group_by(`País de procedencia`) %>%
  count() 

```

here data are a bit dirty, however I don't know if the effort of cleansing them will worth the result.. it depends wheter we decide to use this info in our analysis.

Analyze the epidemic curve separately for each department, considering only those that have more than 30 cases: 

```{r regions, echo=TRUE, warning=FALSE}
covid_dp<-colombia_covid %>%
  group_by(`Fecha de diagnóstico`,`Departamento o Distrito`) %>%
  count()
names(covid_dp)<-c("date", "dep", "n")
#departments with more than 30 cases
relevant<-c("Bogotá D.C.", "Valle del Cauca", "Antioquia", "Cundinamarca", "Risaralda")
covid_relevant_dp<-subset(covid_dp, dep %in% relevant)
covid_relevant_dp
ggplot(covid_relevant_dp, aes(x=date, y=n, fill=dep)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  facet_grid(dep~., scales="free_y") + #every level has a different count axis
  theme_dark() +
  theme(strip.text = element_text(face="bold", size=6))+
  labs(title = "Epidemic curve for different departments",
       subtitle = "considering only dept. with more than 30 cases",
       x = "Date of confirmation",
       y = "count",
       fill = "Department")
```

Analyze the curve of cumulative confirmed cases on those "relevant" department:

```{r cumul_dept, echo=FALSE}
#boring and absolutely not efficient population of the number of cumulative cases
cumulative_dep<-rep(0,83)
covid_relevant_dp<-cbind(covid_relevant_dp, cumulative_dep)
for (i in as.numeric(rownames(covid_relevant_dp[which(covid_relevant_dp$dep=="Bogotá D.C."),]))) {
  if(i==1){
    covid_relevant_dp[i,4]<-covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
  else {
    covid_relevant_dp[i,4]<-prev+covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
}
for (i in as.numeric(rownames(covid_relevant_dp[which(covid_relevant_dp$dep=="Valle del Cauca"),]))) {
  if(i==3){
    covid_relevant_dp[i,4]<-covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
  else {
    covid_relevant_dp[i,4]<-prev+covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
}
for (i in as.numeric(rownames(covid_relevant_dp[which(covid_relevant_dp$dep=="Antioquia"),]))) {
  if(i==2){
    covid_relevant_dp[i,4]<-covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
  else {
    covid_relevant_dp[i,4]<-prev+covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
}
for (i in as.numeric(rownames(covid_relevant_dp[which(covid_relevant_dp$dep=="Cundinamarca"),]))) {
  if(i==13){
    covid_relevant_dp[i,4]<-covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
  else {
    covid_relevant_dp[i,4]<-prev+covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
}
for (i in as.numeric(rownames(covid_relevant_dp[which(covid_relevant_dp$dep=="Risaralda"),]))) {
  if(i==14){
    covid_relevant_dp[i,4]<-covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
  else {
    covid_relevant_dp[i,4]<-prev+covid_relevant_dp[i,3]
    prev<-covid_relevant_dp[i,4]
  }
}
```

```{r plot_cum_dept, echo=TRUE, warning=FALSE}
ggplot(covid_relevant_dp, aes(x=date, y=cumulative_dep, fill=dep)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  facet_grid(dep~., scales="free_y") + #every level has a different count axis
  theme_gray() +
  theme(strip.text = element_text(face="bold", size=6))+
  labs(title = "Cumulative number of cases for different departments",
       subtitle = "considering only dept. with more than 30 cases",
       x = "Date of confirmation",
       y = "count",
       fill = "Department")
```

We can see that (except for the Bogotà department) we have a lot of "missing" columns, these are the days in which no data was recorded for the corresponding department, the cumulative number of cases is the same as the previous day reported in the dataset. Maybe there is a way to fix this!

```{r}
covid_dp <- as.data.frame(covid_dp)
covid_dp$date <- as.Date(covid_dp$date, "%Y-%m-%d")

covid_dp <- covid_dp %>% mutate(BETWEEN0 = as.numeric(difftime(date, lag(date, 1), units = "days")),
            BETWEEN = ifelse(is.na(BETWEEN0), 0, BETWEEN0), elapsed_time = cumsum(as.numeric(BETWEEN))) %>%
            dplyr::select(-c(BETWEEN0,BETWEEN))

write_csv(covid_dp, "data/central_colombia.csv")

covid_relevant_dp <- as.data.frame(covid_relevant_dp)
covid_dp$date <- as.Date(covid_dp$date, "%Y-%m-%d")

covid_relevant_dp <- covid_relevant_dp %>% mutate(BETWEEN0 = as.numeric(difftime(date, lag(date, 1),
                      units = "days")), BETWEEN = ifelse(is.na(BETWEEN0), 0, BETWEEN0), elapsed_time =
                      cumsum(as.numeric(BETWEEN))) %>% dplyr::select(-c(BETWEEN0,BETWEEN))

covid_relevant_dp

write_csv(covid_relevant_dp, "data/central_colombia_relevant.csv")
```

## Missing

I still didn't integrate the "other part" of the dataset, the one concerning deaths!

## Ideas

For what concerns the predictive model we want to build, I think that we should start by something very simple (e.g. a (log)linear model) and take it as a baseline.

Then we build something more complex (such as a hierarchical model) and see the improvements with respect to the baseline.

If possible I would put inside something bayesian, since I understood that they really like this kind of stuff! 

**Gaia** I start here with my stream of consciousness about the analysis! 

Let's fix some terminology (references for these are the first two links of the section "Interesting Links"):

  * a `case` is a person who tested positive for Covid-19;
  * the `epidemic curve` represents the daily incremental incidence;
  
Our data are tabulated by date of confirmation by lab test. 

The variable we want to predict, say `y`, (dependent variable) is the (cumulative) number of confirmed cases, namely we want to simulate a (hopefully plausible) epidemic trajectory. Eventually we will project future incidence. 

Since our `y` is a discrete count variable, a linear regression is not appropriate.

The most common choice with count data is to use **Poisson regression**, which belongs to the GLM class of models.

$$
ln\lambda_i = \beta_0 + \beta_1x_i \\
y_i \sim \mathcal{Poisson}(\lambda_i)
$$

I think that, as first step, we should consider the most parsimonious model, taking only the `time` as independent variable.
Here `time` can be intended both as the `Fecha de diagnostico` attribute of our dataset, or as the number of days elapsed from the earlier day in the dataset (in this case we should derive this attribute).

Further steps will include more covariates, and a model selection phase should follow. At the moment we are basically working with time series data. 

**Important**: recall that modelling count data with a Poisson regression we are assuming *equidispersion* (the mean coincides with the variance), however we have no guarantees that this is true for our data, we need to take this into account!

The days 7/03/20, 8/03/20, 10/03/20 are missing, probably because no case was detected in those days (consistent with the fact that it was the very beginning of the outbreak).

```{r start_analysis, echo=TRUE, warning=FALSE}
cumulative2<-cumulative %>%  
mutate(BETWEEN0=as.numeric(difftime(`Fecha de diagnóstico`, lag(`Fecha de diagnóstico`,1))),BETWEEN=ifelse(is.na(BETWEEN0),0,BETWEEN0), elapsed_time=cumsum(as.numeric(BETWEEN)))%>%
dplyr::select(-c(BETWEEN0,BETWEEN))
#just for praticality
names(cumulative2)<-c("date", "y", "elapsed_time")
cumulative2
```

## Description of variables

**ID de caso.** ID of the confirmed case. 

**Fecha de diagnóstico.** Date that the disease was diagnosed. 

**Ciudad de ubicación.** City where the case was diagnosed.

**Departamento o Distrito.** Department or distric that the city belongs to.

**Atención.** Situation of the pacient: recovered, at home, at the hospital, at the ICU or deceased.

**Edad.** Age of the confirmed case.

**Sexo.** Sex of the confirmed cade.

**Tipo.** How the person got infected: in Colombia, abroad or unknown. 

**País de procedencia.** Country of origin in case the person got infected abroad.



## Country of origin

Cleaning the "`País de procedencia`" variable and creating another variable `Continente de procedencia` related to the continents. As `País de procedencia` is too fragmented is good to group them into continents and see if there is a difference in the model. 

We clean the column `País de procedencia` by making sure that there are no cities instead of countries and that the countries are separated with a dash (we'll use it with dummy variables)

```{r country, echo=TRUE, warning=FALSE}
sort(unique(colombia_covid$`País de procedencia`))

#There is an observation that has a 0 as País de procedencia
which(colombia_covid$`País de procedencia`=="0")
colombia_covid[911,] 

#Standardizing the country names
colombia_covid$`País de procedencia`[colombia_covid$`País de procedencia` == "Isla Martin - Caribe"] <- "Islas San Martin"
colombia_covid$`País de procedencia`[colombia_covid$`País de procedencia` == "Israel Egipto"] <- "Israel - Egipto"
colombia_covid$`País de procedencia`[colombia_covid$`País de procedencia` == "Jamaica - Isla Caimán - Panamá"] <- "Jamaica - Panamá - Isla Caimán"
colombia_covid$`País de procedencia`[colombia_covid$`País de procedencia` == "Madrid"] <- "España"
colombia_covid$`País de procedencia`[colombia_covid$`País de procedencia` == "Londres"] <- "Inglaterra"
colombia_covid$`País de procedencia`[colombia_covid$`País de procedencia` == "Alemania - Estambul"] <- "Alemania - Turquía"
colombia_covid$`País de procedencia`[colombia_covid$`País de procedencia` == "Jamaica - Panamá - Islas del caribe - Cartagena"] <- "Jamaica - Panamá - Colombia"

#Creating continents
Europa <- c("Alemania", "Bélgica", "Europa",  "Croacia", "España", "España - Croacia", "España - Croacia - Bosnia",  "España - Francia", "España - Italia", "Francia", "Francia - Holanda", "Grecia", "Inglaterra", "Italia", "Italia - Ucrania - España", "Suiza")
Asia <- c("Arabia", "Emiratos Árabes", "Turquía")
África <- c("Egipto", "Marruecos")
Norteamérica <- c("Canadá", "Estados Unidos", "México")
Centroamérica <- c("Aruba", "Costa Rica", "Cuba", "Guatemala", "Islas San Martin", "Jamaica", "Jamaica - Panamá - Isla Caimán", "Jamaica - Panamá - Islas del caribe - Cartagena", "Panamá", "Panamá - Jamaica", "Puerto Rico", "República Dominicana")
Sudamerica <- c("Argentina", "Brasil", "Chile", "Ecuador", "Perú", "Venezuela")
# Alemania - Turquía", "España - India", "España - Turquia", "Italia - España - Turquía", "Turquía - Grecia" -> "Europa - Asia"
# "España - Egipto" -> "Europa - África"
# "Israel - Egipto" -> "Asia - África"
# "Italia - Jamaica - Panamá" -> "Europa - Centroamérica"
# "Colombia" -> "Colombia"

for (i in 1:nrow(colombia_covid)) {
  if (colombia_covid$`País de procedencia`[i] %in% Europa){
    colombia_covid$`Continente de procedencia`[i] <- "Europa"}
  else if (colombia_covid$`País de procedencia`[i] %in% Asia){
    colombia_covid$`Continente de procedencia`[i] <- "Asia"}
  else if (colombia_covid$`País de procedencia`[i] %in% África){
    colombia_covid$`Continente de procedencia`[i] <- "África"}
  else if (colombia_covid$`País de procedencia`[i] %in% Norteamérica){
    colombia_covid$`Continente de procedencia`[i] <- "Norteamérica"}
  else if (colombia_covid$`País de procedencia`[i] %in% Centroamérica){
    colombia_covid$`Continente de procedencia`[i] <- "Centroamérica"}
  else if (colombia_covid$`País de procedencia`[i] %in% Sudamerica){
    colombia_covid$`Continente de procedencia`[i] <- "Sudamerica"}
  else if (colombia_covid$`País de procedencia`[i] == "Colombia"){
    colombia_covid$`Continente de procedencia`[i] <- "Colombia"}
  else if (colombia_covid$`País de procedencia`[i] == "Alemania - Turquía"){
    colombia_covid$`Continente de procedencia`[i] <- "Europa - Asia"}
  else if (colombia_covid$`País de procedencia`[i] == "España - India")
    colombia_covid$`Continente de procedencia`[i] <- "Europa - Asia"
  else if (colombia_covid$`País de procedencia`[i] == "España - Turquia")
    colombia_covid$`Continente de procedencia`[i] <- "Europa - Asia"
  else if (colombia_covid$`País de procedencia`[i] == "Italia - España - Turquía")
    colombia_covid$`Continente de procedencia`[i] <- "Europa - Asia"
  else if (colombia_covid$`País de procedencia`[i] == "Turquía - Grecia")
    colombia_covid$`Continente de procedencia`[i] <- "Europa - Asia"
  else if (colombia_covid$`País de procedencia`[i] == "España - Egipto")
    colombia_covid$`Continente de procedencia`[i] <- "Europa - África"
  else if (colombia_covid$`País de procedencia`[i] == "Israel - Egipto")
    colombia_covid$`Continente de procedencia`[i] <- "Asia - África"
  else if (colombia_covid$`País de procedencia`[i] == "Italia - Jamaica - Panamá")
    colombia_covid$`Continente de procedencia`[i] <- "Europa - Centroamérica"
}
# Transforming the 0 value into a null value
library(naniar)
colombia_covid <- colombia_covid %>% replace_with_na(replace = list(`País de procedencia` = 0))
# colombia_covid[911,]$`País de procedencia` <- NA
```

## Preparation of the dataset to run the models

**Giullia.** So, as we have to group the data by date to be able to run the models, we have to count the occurance of the categorical variables in each day. So that's what I did below and described each step in detail. 

Dropping the variables `Ciudad de ubicación`, `Atención**` and `Tipo*`.

Considering the variable "Ciudad de ubicación" I decided to keep just the departments variable to focus the analysis in dividing the region into departments and not in cities.

Considering the variable "Atención**" at the moment I think is not relevant to know how many confirmed cases there will be, but we can add later and check. 

Considering the variable "Tipo*" many observations are "in study", so it is better to use just the variable country of origin (`País de procedencia`), because they consider the ones "in study" as Colombia. 


```{r Giullia1, echo=TRUE, warning=FALSE}
colnames(colombia_covid)[8] <- "Tipo*"
covid19 <- dplyr::select(colombia_covid, -c(`Ciudad de ubicación`,`Atención**`,`Tipo*`))

```

Transforming the age values into age ranges to be able to transform it in a dummy variable

```{r Giullia1a, echo=TRUE, warning=FALSE}
covid19<-covid19 %>% mutate(Edad = case_when(Edad <= 18 ~ "0 a 18",
                                                            Edad >= 19  & Edad <= 30 ~ "19 a 30",
                                                            Edad >=  31 & Edad <= 45 ~ "31 a 45",
                                                            Edad >= 46 & Edad <= 60 ~ "46 a 60",
                                                            Edad >=61 & Edad <= 75 ~ "60 a 75",
                                                            Edad >=76 ~ "76+"))

```

Transform the categorical variables into dummy variables

```{r Giullia1b, echo=TRUE, warning=FALSE}
library(fastDummies)
covid19_dummy <- dummy_cols(covid19, select_columns = c("Departamento o Distrito","Edad","Sexo", "País de procedencia", "Continente de procedencia"),remove_first_dummy = TRUE,ignore_na=TRUE,split="-",remove_selected_columns=TRUE)

```

Counting the occurance of the dummy variables by date and deleting the `Fecha de diagnóstico` and `ID de caso` because I will bind with the cumulative2 table made by Gaia.

```{r Giullia1c, echo=TRUE, warning=FALSE}
group_dummy <- covid19_dummy %>%
  group_by(`Fecha de diagnóstico`) %>%
  summarise_all(funs(sum)) %>%
  dplyr::select(-c(`Fecha de diagnóstico`,`ID de caso`))

```

```{r Giullia1d, echo=TRUE, warning=FALSE}
#Biding

data1 <- bind_cols(cumulative2, group_dummy)
```


## Running Poisson 

### Running poisson with just the variable representing the time as predictor 

```{r Giullia2, echo=TRUE, warning=FALSE}
#Running poisson with just the variable representing the time as predictor 
attach(data1)
poisson1 <- glm(y~elapsed_time,family=poisson)
summary(poisson1)
```

Results show the variable is very significant.

```{r Giullia3, echo=TRUE, warning=FALSE}
#checking for overdispersion
#plotting standardized residuals versus the fitted values
pred.pois <- poisson1$fitted.values
res.st <- (y-pred.pois)/sqrt(pred.pois)

plot(pred.pois, res.st)
abline(h=0,lty=3,col="gray75")

par(mfrow=c(2,2))
plot(poisson1)
```

According to the plot there is presence of overdispersion because the standardized residuals are outside the -1 to 1 range assumed by a Poisson distribution. 
In addition, the residuals are also distributed according to a Poisson. This is surprising and I don't know what does it mean. I have to research.

```{r Giullia4, echo=TRUE, warning=FALSE}
#calculating overdispersion
#n=25 
#k=2
#n-k=23
est.overdispersion <- sum(res.st^2)/23
est.overdispersion
```

Extremely high!!!

```{r}
summary(residuals(poisson1))
poisson1$aic
paste0(c("Null deviance: ", "Residual deviance: "),
       round(c(poisson1$null.deviance, deviance(poisson1)), 2))
```

```{r}
ggplot(data1, aes(elapsed_time, y))+
#    geom_ribbon(aes(x=elapsed_time, ymin=conf.df$lwr, ymax=conf.df$upr),
#                data=data1,
#                fill = color_scheme_get("blue")[[2]])+
    geom_line(aes(x= elapsed_time, y= poisson1$fitted.values),
              data=data1,
              color = color_scheme_get("blue")[[4]],
              size =1.1)+
    geom_point(aes(x = elapsed_time, y = y))+
    xlab("Days")+
    ylab("Total cases")+
    scale_x_discrete(limit = c( 8, 15, 22, 29, 36 ), 
        labels = c( "2-3", "9-3", "16-3", "23-3", "30-3") )+
    #facet_wrap("reg", scales ="free")+
    theme(strip.text.x = element_text(size = 12, 
          colour = "black"),
          axis.text.x = element_text(face="bold", 
          color="#993333", 
          angle=45, size =9),
          axis.title.x = element_text(size=22),
          axis.title.y = element_text(size = 22))
```


### Stan model just to compare

```{r stan1}
model.Poisson<-stan_model("stan/poisson_regression.stan")
#arrange things
model.data<-list(
  N = nrow(data1),
  cases = data1$y,
  time = data1$elapsed_time
)
#str(model.Poisson.data)

#run the model 
fit.model.Poisson<-sampling(model.Poisson, data=model.data)

#inferred parameters
print(fit.model.Poisson, pars=c("alpha", "beta"))

y_rep<-as.matrix(fit.model.Poisson, pars="y_rep")
ppc_dens_overlay(y = model.data$cases, y_rep[1:200,]) 
```

Let's apply a quasi poisson and see what happens

```{r Giullia5, echo=TRUE, warning=FALSE}
poisson1quasi <- glm(y~elapsed_time,family=quasipoisson)
summary(poisson1quasi)

#plotting standardized residuals versus the fitted values
pred.poisq <- poisson1quasi$fitted.values
res.stq <- (y-pred.poisq)/sqrt(summary(poisson1quasi)$dispersion*pred.poisq)
plot(pred.poisq, res.stq)
abline(h=0,lty=3,col="gray75")

#calculating overdispersion
est.overdispersion <- sum(res.stq^2)/23
est.overdispersion
```

So I applied the quasi poisson and the overdispersion reduced to 1. Anyway, the residuals are still behaving like a distribution. Have to understand why is that.

### Running poisson with the variable representing time plus gender

```{r Giullia6, echo=TRUE, warning=FALSE}
#Running poisson with the variable representing time plus gender
poisson2 <- glm(y~elapsed_time+Sexo_M,family=poisson)
summary(poisson2)
```

Results show that the variable gender is not signficant and thus doesn't change the performance of the model. We were already expecting this as in the data visualization step the gender is equaly distributed across the confirmed cases.

```{r Giullia6b, echo=TRUE, warning=FALSE}
#Running poisson with the variable representing time plus gender
#poisson2b <- glm(`Fecha de diagnóstico` ~ `C` + Sexo_M, data=colombia_covid, family=poisson)
#summary(poisson2b)
```


### Running poisson with the variable representing time plus the age variables

```{r Giullia7, echo=TRUE, warning=FALSE}
#Running poisson with the variable representing time plus the age variables
poisson3 <- glm(y~elapsed_time+`Edad_19 a 30`+`Edad_31 a 45`+`Edad_46 a 60`+`Edad_60 a 75`+`Edad_76+`,family=poisson)
summary(poisson3)
```

Results show that 4 out of the 5 age ranges are significant predictors, showing that age does have an effect in predicting the number of confirmed cases.
Furthermore, it can be seen that the AIC reduced in comparison with the model that has just the variable time as predictor (poisson1).

Plotting the residuals versus fit and calculating the overdispersion

```{r Giullia8, echo=TRUE, warning=FALSE}
pred.pois3 <- poisson3$fitted.values
res.st3 <- (y-pred.pois3)/sqrt(pred.pois3)
plot(pred.pois3, res.st3)
abline(h=0,lty=3,col="gray75")

#calculating overdispersion
#n=25 
#k=7
#n-k=18
est.overdispersion <- sum(res.st3^2)/18
est.overdispersion
```

The overdispersion is high and again the residuals continue to behave like a distribution.

```{r}
predict.confidence <- function(object, newdata, level = 0.95, ...) {
    if (!is(object, "glm")) {
        stop("Model should be a glm")
    }
    if (!is(newdata, "data.frame")) {
        stop("Plase input a data frame for newdata")
    }
    if (!is.numeric(level) | level < 0 | level > 1) {
        stop("level should be numeric and between 0 and 1")
    }
    ilink <- family(object)$linkinv
    ci.factor <- qnorm(1 - (1 - level)/2)
    # calculate CIs:
    fit <- predict(object, newdata = newdata, level = level, 
                    type = "link", se.fit = TRUE, ...)
    lwr <- ilink(fit$fit - ci.factor * fit$se.fit)
    upr <- ilink(fit$fit + ci.factor * fit$se.fit)
    df <- data.frame("fit" = ilink(fit$fit), "lwr" = lwr, "upr" = upr)
    return(df)
}

conf.df <- predict.confidence(poisson3, as.data.frame(data1$y))
conf.df
```

```{r plot_predict}
plot(conf.df$fit, col="blue")
points(conf.df$lwr, col="gray")
points(conf.df$upr, col="gray")
```

```{r}
ggplot(data1, aes(elapsed_time, y))+
    geom_ribbon(aes(x=elapsed_time, ymin=conf.df$lwr, ymax=conf.df$upr),
                data=data1,
                fill = color_scheme_get("blue")[[2]])+
    geom_line(aes(x= elapsed_time, y= conf.df$fit),
              data=data1,
              color = color_scheme_get("blue")[[4]],
              size =1.1)+
    geom_point(aes(x = elapsed_time, y = y))+
    xlab("Days")+
    ylab("Total cases")+
    scale_x_discrete(limit = c( 8, 15, 22, 29, 36 ), 
        labels = c( "2-3", "9-3", "16-3", "23-3", "30-3") )+
    #facet_wrap("reg", scales ="free")+
    theme(strip.text.x = element_text(size = 12, 
          colour = "black"),
          axis.text.x = element_text(face="bold", 
          color="#993333", 
          angle=45, size =9),
          axis.title.x = element_text(size=22),
          axis.title.y = element_text(size = 22))
```


```{r Giullia9, echo=TRUE, warning=FALSE}
#Let's apply a quasi poisson and see what happens
poisson3quasi <- glm(y~elapsed_time+`Edad_19 a 30`+`Edad_31 a 45`+`Edad_46 a 60`+`Edad_60 a 75`+`Edad_76+`,family=quasipoisson)
summary(poisson3quasi)
```

When we apply the quasi poisson model the age variables become not signficant anymore. But we know they are. So, I think is not a problem.

```{r Giullia9a, echo=TRUE, warning=FALSE}
#plotting standardized residuals versus the fitted values
pred.poisq3 <- poisson3quasi$fitted.values
res.stq3 <- (y-pred.poisq3)/sqrt(summary(poisson3quasi)$dispersion*pred.poisq3)
plot(pred.poisq3, res.stq3)
abline(h=0,lty=3,col="gray75")

#calculating overdispersion
est.overdispersion <- sum(res.stq3^2)/18
est.overdispersion
```

By applying the quasi poisson the overdispersion reduced to 1, and again the residuals are still behaving like a distribution.

### Running poisson with the variable representing time, age and departments as predictors

```{r Giullia10, echo=TRUE, warning=FALSE}
#Running poisson with the variable representing time, age and departments as predictors
poisson4 <- glm(y~elapsed_time+`Edad_19 a 30`+`Edad_31 a 45`+`Edad_46 a 60`+`Edad_60 a 75`+`Edad_76+`+`Departamento o Distrito_Bogotá D.C.`+`Departamento o Distrito_Boyacá`+`Departamento o Distrito_Caldas`+`Departamento o Distrito_Casanare`+`Departamento o Distrito_Cauca`+`Departamento o Distrito_Cundinamarca`+`Departamento o Distrito_Meta`+`Departamento o Distrito_Quindío`+`Departamento o Distrito_Risaralda`+`Departamento o Distrito_Santander`+`Departamento o Distrito_Tolima`+`Departamento o Distrito_Valle del Cauca`, family=poisson)
summary(poisson4)
```

The results show that by including the variables representing departments the AIC reduces significantly in comparison with the previous model that included just time and age as predictors. In addition, 8 out of 12 of the dummy variables representing the departments are significant. 


```{r Giullia11, echo=TRUE, warning=FALSE}
#plotting the residuals versus fit to analyze the overdispersion 
pred.pois4 <- poisson4$fitted.values
res.st4 <- (y-pred.pois4)/sqrt(pred.pois4)
plot(pred.pois4, res.st4)
abline(h=0,lty=3,col="gray75")

#calculating overdispersion
#n=25 
#k=19
#n-k=6
est.overdispersion <- sum(res.st4^2)/6
est.overdispersion
```

Now the residuals are behaving differently and the overdispersion reduced significantly and it is inside the range assumed by a Poisson model, that is from -1 to 1. Therefore, there is no need to apply a quasi poisson model because the assumptions of the Poisson distribution are holding.

### Running poisson with the variable representing time, age, departments and continent of origin as predictors

```{r Giullia12, echo=TRUE, warning=FALSE}
#Running poisson with the variable representing time, age, departments and continent of origin as predictors
poisson5 <- glm(y~elapsed_time+`Edad_19 a 30`+`Edad_31 a 45`+`Edad_46 a 60`+`Edad_60 a 75`+`Edad_76+`+`Departamento o Distrito_Bogotá D.C.`+`Departamento o Distrito_Boyacá`+`Departamento o Distrito_Caldas`+`Departamento o Distrito_Casanare`+`Departamento o Distrito_Cauca`+`Departamento o Distrito_Cundinamarca`+`Departamento o Distrito_Meta`+`Departamento o Distrito_Quindío`+`Departamento o Distrito_Risaralda`+`Departamento o Distrito_Santander`+`Departamento o Distrito_Tolima`+`Departamento o Distrito_Valle del Cauca`+`Continente de procedencia_Asia`+`Continente de procedencia_Centroamérica`+`Continente de procedencia_Colombia`+`Continente de procedencia_Europa`+`Continente de procedencia_Norteamérica`+`Continente de procedencia_Sudamerica`, family=poisson)
summary(poisson5)
```

The AIC actually increased by adding the continent of origin variables and none of them are significant.

```{r Giullia13, echo=TRUE, warning=FALSE}
#Let's confirm if the continent of origin variable is not significant at all when being a predictor just together with the time variable.
poisson6 <- glm(y~elapsed_time+`Continente de procedencia_Asia`+`Continente de procedencia_Centroamérica`+`Continente de procedencia_Colombia`+`Continente de procedencia_Europa`+`Continente de procedencia_Norteamérica`+`Continente de procedencia_Sudamerica`, family=poisson)
summary(poisson6)
```

In this case the AIC reduced in comparison with the model with just the time as predictor(poisson1), but just 2 out of 6 variables are significant, so we think is not a very relevant predictor to include in the model with the lowest AIC so far (poisson4).

### Applying ANOVA to compare the poisson models

We decided to compare poisson1quasi, poisson3quasi, poisson4, because they are nested and we are more interested in seeing if the model 4 is in fact better than the first model.

```{r Giullia13a, echo=TRUE, warning=FALSE}
anova(poisson1quasi,poisson3quasi,poisson4, test="Chisq")
```

```{r Angela11, echo=TRUE, warning=FALSE}
anova(poisson1,poisson3,poisson4, poisson5, test="Chisq")
```

The poisson4 model is the best. It has the lower p value.

Two important observations **Angela**. 

- Is it ok that we compare together a poisson with a quasi poisson?

- The problem I said I am having is that you can see that the table is giving me just the p value. Is missing other metrics of the test. Go to the comparison of the negative binomial models and you will see that there it is working properly. Is something related to the Poisson model on our data. If I compare just poisson models or just quasi poisson models it continues to give the same problem. 


## Running Negative Binomial

### Running negative binomial with just the variable representing the time as predictor

```{r Giullia14, echo=TRUE, warning=FALSE}
#Running negative binomial with just the variable representing the time as predictor
nb1 <- glm.nb(y~elapsed_time)
summary(nb1)
```

The predictor is significant and we get an AIC significantly lower than the poisson model runned with the same predictor.

```{r Giullia15, echo=TRUE, warning=FALSE}
#Plotting standardized residuals vs fitted values
stdres <- rstandard(nb1)
plot(nb1$fitted.values, stdres)
abline(h=0, col='gray')
```

The residuals are just in the lowerbound exceeding by approximately one unit the -1 to 1 variation range. 

```{r Giullia16, echo=TRUE, warning=FALSE}
#Calculating overdispersion
#n=25 
#k=2
#n-k=23
est.overdispersion <- sum(stdres^2)/23
est.overdispersion
```

The overdispersion is bit higher than 1, but I think it is still an acceptable number. Gioia said than when it gets closer to 2 that you can accept the overdispersion assumption.

```{r stan2}
model.NB<-stan_model("stan/negative_binomial.stan")
#arrange things
model.data<-list(
  N = nrow(data1),
  cases = data1$y,
  time = data1$elapsed_time
)
#str(model.Poisson.data)

#run the model 
fit.model.NB <- sampling(model.NB, data=model.data)

#inferred parameters
print(fit.model.NB, pars=c("alpha", "beta"))

y_rep<-as.matrix(fit.model.NB, pars="y_rep")
ppc_dens_overlay(y = model.data$cases, y_rep[1:200,]) 
```

### Running negative binomial with the variable representing time plus the age variables as predictors

```{r Giullia17, echo=TRUE, warning=FALSE}
#Running negative binomial with the variable representing time plus the age variables as predictors
nb2 <- glm.nb(y~elapsed_time+`Edad_19 a 30`+`Edad_31 a 45`+`Edad_46 a 60`+`Edad_60 a 75`+`Edad_76+`)
summary(nb2)
```

The AIC increased in comparison with the previous model(nb1) and the age variables are not significant. That's awkward. In the Poisson model they are.

### Running negative binomial with the variable representing time plus the departments variables as predictors

```{r Giullia18, echo=TRUE, warning=FALSE}
#Running negative binomial with the variable representing time plus the departments variables as predictors
nb3 <- glm.nb(y~elapsed_time+`Departamento o Distrito_Bogotá D.C.`+`Departamento o Distrito_Boyacá`+`Departamento o Distrito_Caldas`+`Departamento o Distrito_Casanare`+`Departamento o Distrito_Cauca`+`Departamento o Distrito_Cundinamarca`+`Departamento o Distrito_Meta`+`Departamento o Distrito_Quindío`+`Departamento o Distrito_Risaralda`+`Departamento o Distrito_Santander`+`Departamento o Distrito_Tolima`+`Departamento o Distrito_Valle del Cauca`)
summary(nb3)
```

The AIC decreased in comparison with the first model(nb1), but very littlle. In addition, just 5 out of the 12 department dummy variables are significant.

### Running negative binomial with the variable representing time plus the continent of origin variables as predictors

```{r Giullia19, echo=TRUE, warning=FALSE}
#Running negative binomial with the variable representing time plus the continent of origin variables as predictors
nb4 <- glm.nb(y~elapsed_time+`Continente de procedencia_Asia`+`Continente de procedencia_Centroamérica`+`Continente de procedencia_Colombia`+`Continente de procedencia_Europa`+`Continente de procedencia_Norteamérica`+`Continente de procedencia_Sudamerica`)
summary(nb4)

```

The AIC decreased in comparison with the first model(nb1), but very littlle. In addition, just one out of the 6 department dummy variables is significant.

### Running negative binomial with the variable representing time plus the time, age and departments as pedictors

```{r Angela, echo=TRUE, warning=FALSE}
#Running negative binomial with the variable representing time, age and departments as predictors
nb5 <- glm.nb(y~elapsed_time+`Edad_19 a 30`+`Edad_31 a 45`+`Edad_46 a 60`+`Edad_60 a 75`+`Edad_76+`+`Departamento o Distrito_Bogotá D.C.`+`Departamento o Distrito_Boyacá`+`Departamento o Distrito_Caldas`+`Departamento o Distrito_Casanare`+`Departamento o Distrito_Cauca`+`Departamento o Distrito_Cundinamarca`+`Departamento o Distrito_Meta`+`Departamento o Distrito_Quindío`+`Departamento o Distrito_Risaralda`+`Departamento o Distrito_Santander`+`Departamento o Distrito_Tolima`+`Departamento o Distrito_Valle del Cauca`)
summary(nb5)
```

When the variables age and departments are both included in the model, the AIC reduces in comparison to the model with the lowest AIC so far(nb1).

```{r Angela2, echo=TRUE, warning=FALSE}
#Plotting standardized residuals vs fitted values
stdres <- rstandard(nb5)
plot(nb5$fitted.values, stdres)
abline(h=0, col='gray')
```

```{r Angela3, echo=TRUE, warning=FALSE}
# Calculating overdispersion n=25 k=19 n-k=6
est.overdispersion <- sum(stdres^2)/6
est.overdispersion
```

Here the overdispersion is a bit high. **Angela**, do you know if in the negative binomial the assumptions of equidispersion are the same of Poisson? which are:

- Overdispersion has to be close to one .

- The range of the standardized residual has to be between -1 and 1.

### Applying ANOVA to compare the negative binomial models

We decided to compare nb1, nb2, nb5, because they are nested and we are more interested in seeing if the model 5 is in fact better than the first model.

```{r Giullia20, echo=TRUE, warning=FALSE}
#Applying ANOVA to compare the negative binomial models
anova(nb1,nb2,nb5)
```

The fifth model is definetly the best, is the one with the lowest p value.

## Preliminary conclusions

In the case of the Poisson, the model with the lower AIC (203.51) includes 3 variables as predictor: time, age and departments

In the case of the Negative binomial, the model with the lower AIC (205.51) includes also the same 3 variables of the best Poisson model. 

## Predictive accuracy

Calculating the predictive accuracy of the best model of Poisson and the best model of the Negative Binomial. 

### Predictive accuracy of the Poisson model

Predicting with a 95% confidence interval

```{r Giullia21, echo=TRUE, warning=FALSE}
conf.df <- predict.confidence(poisson4, as.data.frame(y))
conf.df
```

Calculating the proportion of times that the true value of `y` is in the 95% confidence interval of our model. (This is a kind of accuracy I suppose right?)  

```{r Giullia22, echo=TRUE, warning=FALSE}
n <- 25
freq_coverage <- sum( y >= conf.df[,2] & y <= conf.df[,3])
freq_coverage <- freq_coverage/n
freq_coverage
```

We have a 92% of coverage!!! I think this is extremely good, even if it has been predicted in the training data. Because this doesn't necessarily happens.

```{r Giullia23, echo=TRUE, warning=FALSE}
#plotting the observed y (blue dots) versus the confidence interval (gray dots)
plot(y, col = "blue")
points(conf.df$lwr, col = "gray")
points(conf.df$upr, col = "gray")
```

```{r Giullia24, echo=TRUE, warning=FALSE}
#plotting the prediction line (straight blue line) with a 95% confidence interval (shaded blue line) together with the observed values (black dots)
ggplot(data1, aes(elapsed_time, y)) + geom_ribbon(aes(x = elapsed_time, ymin = conf.df$lwr, 
    ymax = conf.df$upr), data = data1, fill = color_scheme_get("blue")[[2]]) + 
    geom_line(aes(x = elapsed_time, y = conf.df$fit), data = data1, color = color_scheme_get("blue")[[4]], 
        size = 1.1) + geom_point(aes(x = elapsed_time, y = y)) + xlab("Days") + 
    ylab("Total cases") + scale_x_discrete(limit = c(8, 15, 22, 29, 36), labels = c("2-3", 
    "9-3", "16-3", "23-3", "30-3")) + # facet_wrap('reg', scales ='free')+
theme(strip.text.x = element_text(size = 12, colour = "black"), axis.text.x = element_text(face = "bold", 
    color = "#993333", angle = 45, size = 9), axis.title.x = element_text(size = 22), 
    axis.title.y = element_text(size = 22))
```

We have almost like a perfect model!!!

But now let's perform a leave-one-out cross validation to see what result we get. (obs: I didn't find the code to predict with a 95% interval, so I am just calculating the overall prediction error)

```{r LOOCV, echo=TRUE, warning=FALSE}
library(boot)
cv.glm(data1,poisson4)$delta
```

I am not being able to understand this error number. Is it too high? I will research tomorrow. I know the one in the left is the standard error and the one in the right is the adjusted error. 

I runned again a leane-one-out cross-validation with the other two Poisson models

```{r Giullia29, echo=TRUE, warning=FALSE}
cv.glm(data1,poisson3quasi)$delta
cv.glm(data1,poisson1quasi)$delta
```

So if you compare poisson4 with these other two models you can see that as you add more variables the test error decreases. So we should choose the model just with time as predictor (poisson1quasi)??? Even if the model with many predictors produces a better AIC and the variables are statistical significant? I don't know what to do.

### Predictive accuracy of the Negative Binomial model

Predicting with a 95% confidence interval

```{r Giullia25, echo=TRUE, warning=FALSE}
conf.df <- predict.confidence(nb5, as.data.frame(y))
conf.df

```

Calculating the proportion of times that the true value of `y` is in the 95% confidence interval of our model. (This is a kind of accuracy I suppose right?)  

```{r Giullia26, echo=TRUE, warning=FALSE}
n <- 25
freq_coverage <- sum( y >= conf.df[,2] & y <= conf.df[,3])
freq_coverage <- freq_coverage/n
freq_coverage
```

We also obtain a 92% of coverage for the Negative Binomial model.

```{r Giullia27, echo=TRUE, warning=FALSE}
#plotting the observed y (blue dots) versus the confidence interval (gray dots)
plot(y, col = "blue")
points(conf.df$lwr, col = "gray")
points(conf.df$upr, col = "gray")
```

```{r Giullia28, echo=TRUE, warning=FALSE}
#plotting the prediction line (straight blue line) with a 95% confidence interval (shaded blue line) together with the observed values (black dots)
ggplot(data1, aes(elapsed_time, y)) + geom_ribbon(aes(x = elapsed_time, ymin = conf.df$lwr, 
    ymax = conf.df$upr), data = data1, fill = color_scheme_get("blue")[[2]]) + 
    geom_line(aes(x = elapsed_time, y = conf.df$fit), data = data1, color = color_scheme_get("blue")[[4]], 
        size = 1.1) + geom_point(aes(x = elapsed_time, y = y)) + xlab("Days") + 
    ylab("Total cases") + scale_x_discrete(limit = c(8, 15, 22, 29, 36), labels = c("2-3", 
    "9-3", "16-3", "23-3", "30-3")) + # facet_wrap('reg', scales ='free')+
theme(strip.text.x = element_text(size = 12, colour = "black"), axis.text.x = element_text(face = "bold", 
    color = "#993333", angle = 45, size = 9), axis.title.x = element_text(size = 22), 
    axis.title.y = element_text(size = 22))
```

Also a very precise model.

But now let's perform a leave-one-out cross validation to see what result we get. (obs: I didn't find the code to predict with a 95% interval, so I am just calculating the overall prediction error)

```{r LOOCV1, echo=TRUE, warning=FALSE}
cv.glm(data1,nb5)$delta
```

Approximately the same result as the Poisson model. 

I runned again a leane-one-out cross-validation with the other two Negative Binomial models.

```{r Giullia29a, echo=TRUE, warning=FALSE}
cv.glm(data1,nb2)$delta
cv.glm(data1,nb1)$delta
```

Here happens the same thing that happened in Poisson. If you compare nb5 with these other two models you can see that as you add more variables the test error decreases. So again we should choose the model just with time as predictor (nb1)??? Even if the model with many predictors produces a better AIC and the variables are statistical signficant? Again, I don’t know what to do.

## Final conclusions

I thought that with the predictive accuracy we would be able to choose between the poisson and the negative binomial. So this didn't happen. They perform equally, both when we predict with the 95% confidence interval in the training data and in the leave-one-out cross validation. And now there is the problem of the overfitting. So what do we do now?



```{r Giullia30, echo=TRUE, warning=FALSE}
```

```{r Giullia31, echo=TRUE, warning=FALSE}
```

```{r Giullia32, echo=TRUE, warning=FALSE}
```

## Interesting links

https://timchurches.github.io/blog/posts/2020-02-18-analysing-covid-19-2019-ncov-outbreak-data-with-r-part-1/

https://timchurches.github.io/blog/posts/2020-03-01-analysing-covid-19-2019-ncov-outbreak-data-with-r-part-2/

http://freerangestats.info/blog/2020/05/09/covid-population-incidence

[interpetation glm](https://www.datascienceblog.net/post/machine-learning/interpreting_generalized_linear_models/)

<!-- knitr::knit("project.Rmd", tangle = TRUE, output ="project.R") -->